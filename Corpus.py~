#generate bag of words

import nltk
from nltk import *
import gensim
from gensim import *
import csv
from collections import defaultdict
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
import collections
from nltk.stem.wordnet import WordNetLemmatizer

columns = defaultdict(list)

with open("BUS_new",'r') as f:
  reader=csv.DictReader(f)
  for row in reader:    
    for(k,v) in row.items():
      columns[k].append(v)


en_stop = get_stop_words('en')
tokenizer = RegexpTokenizer(r'\w+')
p_stemmer = PorterStemmer()
wordnet_lemmatizer = WordNetLemmatizer()
text_tak=[]

for a,b,c,d in zip(columns['Title'],columns['Abstract'],columns['Index Keywords'],columns['Author Keywords']):
  text_tak.append((str(a)+str(b)+str(c)+str(d)))

texts = []

for i in text_tak:
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)
    stopped_tokens = [i for i in tokens if not i in en_stop]
    lem_tokens=[wordnet_lemmatizer.lemmatize(i) for i in stopped_tokens]
    #stemmed_tokens = [p_stemmer.stem(i) for i in lem_tokens]
    fdist = nltk.FreqDist(lem_tokens)
    fin_tokens = [w for w in lem_tokens if len(w)> 2 and fdist[w] > 1]
    texts.append(fin_tokens)
    
dictionary = corpora.Dictionary(texts)
dict_saved=dictionary.save('dictionary_bus_new.dict')

#dict_f=open('dicitonary.txt','w')
#for k,v in (collections.OrderedDict(sorted(dictionary.items()))).iteritems():
#  print k,v

#print dictionary
corpus = [dictionary.doc2bow(text) for text in texts]
corpus_saved=corpora.MmCorpus.serialize('corpus_bus_new.mm', corpus)



